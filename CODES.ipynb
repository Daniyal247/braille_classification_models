{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587ed367",
   "metadata": {},
   "source": [
    "# Epoch 54/200\n",
    "814/814 [==============================] - 31s 38ms/step - loss: 0.2906 - accuracy: 0.9395 - val_loss: 0.7012 - val_accuracy: 0.8555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model_ckpt = ModelCheckpoint('dataModel.h5', save_best_only=True)\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=20,\n",
    "                             shear_range=10,\n",
    "                             validation_split=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('/Users/syeddaniyalkhurram/Documents/model_file/charsOnly/',\n",
    "                                              target_size=(28, 28),\n",
    "                                              subset='training',\n",
    "                                              batch_size=18,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "val_generator = datagen.flow_from_directory('/Users/syeddaniyalkhurram/Documents/model_file/charsOnly/',\n",
    "                                            target_size=(28, 28),\n",
    "                                            subset='validation',\n",
    "                                            batch_size=18,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "entry = L.Input(shape=(28, 28, 3))\n",
    "x = L.Conv2D(64,(3,3), strides=2,padding='same' ,activation='relu')(entry)\n",
    "x = L.Conv2D(64,(3,3),padding='same' ,activation='relu')(entry)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2D(128,(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "x = L.Conv2D(128,(3,3), padding='same', activation='relu')(x)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2D(256,(2,2), strides=2, padding='same', activation='relu')(x)\n",
    "x = L.Conv2D(256,(2,2), padding='same', activation='relu')(x)\n",
    "x = L.MaxPooling2D()(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2DTranspose(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = L.Conv2DTranspose(256, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = L.Conv2DTranspose(128, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = L.Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "x = L.GlobalMaxPooling2D()(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(64, activation='relu')(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(32,kernel_regularizer=l2(2e-4))(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dense(26,activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(entry, x)\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = model.fit(train_generator, epochs=200, validation_data=val_generator, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc259d2",
   "metadata": {},
   "source": [
    "# Epoch 74/200\n",
    "727/727 [==============================] - 11s 15ms/step - loss: 0.2259 - accuracy: 0.9375 - val_loss: 0.7433 - val_accuracy: 0.8089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model_ckpt = ModelCheckpoint('dataModel.h5', save_best_only=True)\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=20,\n",
    "                             shear_range=20,\n",
    "                             validation_split=0.2,\n",
    "                             zoom_range=0.4,\n",
    "                             rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('/Users/syeddaniyalkhurram/Documents/model_file/LL/resized28/',\n",
    "                                              target_size=(28, 28),\n",
    "                                              subset='training',\n",
    "                                              batch_size=20,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "val_generator = datagen.flow_from_directory('/Users/syeddaniyalkhurram/Documents/model_file/LL/resized28/',\n",
    "                                            target_size=(28, 28),\n",
    "                                            subset='validation',\n",
    "                                            batch_size=20,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "entry = L.Input(shape=(28, 28, 3))\n",
    "x = L.Conv2D(64,(3,3), strides=2,padding='same' ,activation='relu')(entry)\n",
    "x = L.Conv2D(64,(3,3),padding='same' ,activation='relu')(entry)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2D(128,(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "x = L.Conv2D(128,(3,3), padding='same', activation='relu')(x)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2D(256,(2,2), strides=2, padding='same', activation='relu')(x)\n",
    "x = L.Conv2D(256,(2,2), padding='same', activation='relu')(x)\n",
    "x = L.GlobalMaxPooling2D()(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(256, activation='relu')(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dropout(0.25)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(64,kernel_regularizer=l2(2e-4))(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dense(26,activation='softmax')(x)\n",
    "\n",
    "model = Model(entry, x)\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = model.fit(train_generator, epochs=200, validation_data=val_generator, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344e5c8",
   "metadata": {},
   "source": [
    "# Epoch 45/200\n",
    "429/429 [==============================] - 29s 69ms/step - loss: 0.0610 - accuracy: 0.9859 - val_loss: 0.7295 - val_accuracy: 0.8499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b396ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model_ckpt = ModelCheckpoint('dataModel.h5', save_best_only=True)\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=5,\n",
    "                             shear_range=10,\n",
    "                             validation_split=0.25,\n",
    "                             zoom_range=0.2,\n",
    "                             rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('/Users/syeddaniyalkhurram/Documents/model_file/LL/Letters1/',\n",
    "                                              target_size=(28, 28),\n",
    "                                              subset='training',\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "val_generator = datagen.flow_from_directory('/Users/syeddaniyalkhurram/Documents/model_file/LL/Letters1/',\n",
    "                                            target_size=(28, 28),\n",
    "                                            subset='validation',\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "entry = L.Input(shape=(28, 28, 3))\n",
    "x = L.Conv2D(64,(3,3), strides=2,padding='same' ,activation='relu')(entry)\n",
    "x = L.Conv2D(64,(3,3),padding='same' ,activation='relu')(entry)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2D(128,(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "x = L.Conv2D(128,(3,3), padding='same', activation='relu')(x)\n",
    "x = L.MaxPooling2D((2,2))(x)\n",
    "x = L.Dropout(0.3)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Conv2D(256,(2,2), strides=2, padding='same', activation='relu')(x)\n",
    "x = L.Conv2D(256,(2,2), padding='same', activation='relu')(x)\n",
    "x = L.GlobalMaxPooling2D()(x)\n",
    "x = L.Dropout(0.3)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.Dense(64, activation='relu')(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dense(64,kernel_regularizer=l2(2e-4))(x)\n",
    "x = L.LeakyReLU()(x)\n",
    "x = L.Dense(26,activation='softmax')(x)\n",
    "\n",
    "model = Model(entry, x)\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = model.fit(train_generator, epochs=200, validation_data=val_generator, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
